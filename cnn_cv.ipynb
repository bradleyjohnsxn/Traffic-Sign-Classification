{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c55382e5-4dbc-4504-b7c4-9bb6e7e4c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import interpolate\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198cbfb9-8384-405b-b338-87fc5baba929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reshape images\n",
    "def change_dataset(changed, dataset, BS):\n",
    "    ind = min(len(dataset),BS)\n",
    "    X = np.zeros((ind,300,300,3))\n",
    "    \n",
    "    if ind != len(dataset):\n",
    "        X = dataset[:ind]\n",
    "        X = X.reshape((ind,300,300,3))\n",
    "        res = np.zeros((X.shape[0],3,300,300))\n",
    "        for i in range(X.shape[0]):\n",
    "            res[i] = np.moveaxis(X[i],-1,0)\n",
    "        res = interpolate(torch.tensor(res),size=(175,175),mode='bicubic',align_corners=False)\n",
    "        \n",
    "        if len(changed) == 1:\n",
    "            changed = np.vstack([changed,np.array(res)])\n",
    "            changed = changed [1:]\n",
    "        else:\n",
    "            changed = np.vstack([changed,np.array(res)])\n",
    "        dataset = dataset[ind:]\n",
    "        \n",
    "    else:\n",
    "        X = dataset\n",
    "        X = X.reshape((ind,300,300,3))\n",
    "        res = np.zeros((X.shape[0],3,300,300))\n",
    "        for i in range(X.shape[0]):\n",
    "            res[i] = np.moveaxis(X[i],-1,0)\n",
    "        res = interpolate(torch.tensor(res),size=(175,175),mode='bicubic',align_corners=False)\n",
    "        changed = np.vstack([changed,np.array(res)])\n",
    "        dataset = np.array([])\n",
    "    return changed, dataset\n",
    "\n",
    "def min_max_scale(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i] = (data[i]-np.min(data[i]))/(np.max(data[i])-np.min(data[i]))\n",
    "    return data\n",
    "\n",
    "\n",
    "# define training hyperparameters\n",
    "BS = 128\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bdd11bc-396c-4551-b70c-fd533d53acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.shape, labels.shape: ((270000, 6195), (6195,))\n",
      "2. dataset.shape, labels.shape: ((6195, 3, 175, 175), (6195,))\n",
      "3. X_train.shape, labels.shape: ((6195, 3, 175, 175), (6195,))\n"
     ]
    }
   ],
   "source": [
    "# ----------- PREPARING DATA ------------- #\n",
    "# loading Data\n",
    "dataset = np.load('../share/data_train.npy')\n",
    "t_train = np.load('../share/labels_train.npy')\n",
    "print(f'dataset.shape, labels.shape: {dataset.shape, t_train.shape}')\n",
    "\n",
    "\n",
    "# fixing mislabeled data\n",
    "location = [880, 165, 558, 2396, 3198, 3715, 3734, 3824, 4412, 5238, 38, 2303, 3127, 3467, 3515, 5404, 6178, 248, 827, 1139, 1799, 2491, 2546, 4533, 6024, 103, 392, 846, 913, 1487, 2971, 3234, 3851, 4008, 4405, 4442, 4952, 5348, 204, 272, 297, 1177, 1198, 1326, 1655, 2156, 2690, 2864, 3450, 5095, 5111, 5186, 5243, 5785, 6106,387, 3973, 5112, 5331, 5002, 5263, 6046, 1843, 2941, 3419, 4158, 4166, 4369, 4980, 689, 973, 3066, 3028, 4905, 5107, 5708, 5817, 357, 368, 1065, 2093, 3229, 3341, 4469, 5257, 5711]\n",
    "true_label = [6, 4, 7, 9, 5, 6, 3, 8, 2, 0, 5, 1, 4, 6, 3, 7, 9, 2, 7, 2, 8, 5, 1, 0, 6, 5, 2, 5, 6, 0, 5, 5, 5, 8, 5, 2, 5, 1, 4, 4, 4, 1, 4, 7, 0, 4, 4, 4, 9, 4, 4, 4, 8, 4, 0, 8, 3, 5, 2, 5, 0, 4, 6, 8, 0, 6, 9, 5, 2, 0, 3, 6, 7, 2, 5, 7, 1, 2, 7, 0, 5, 8,3, 1, 6, 8]\n",
    "\n",
    "for k in range(len(location)):\n",
    "    t_train[location[k]] = true_label[k]\n",
    "\n",
    "# downsample data\n",
    "dataset = dataset.T\n",
    "X_train = np.zeros((1,3,175,175), dtype = np.float32)\n",
    "while len(dataset) != 0:\n",
    "    X_train, dataset = change_dataset(X_train,dataset,64)\n",
    "print(f'2. dataset.shape, labels.shape: {X_train.shape, t_train.shape}')\n",
    "\n",
    "# min-max scaling\n",
    "X_train = X_train.reshape((len(X_train), 3*175*175))\n",
    "mms = MinMaxScaler()\n",
    "X_train = mms.fit_transform(X_train)\n",
    "X_train = X_train.reshape((len(X_train),3,175,175))\n",
    "print(f'3. X_train.shape, labels.shape: {X_train.shape, t_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc83d49f-99be-4798-81d8-056a8eeec943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn class with variable inputs\n",
    "class cnn(nn.Module):\n",
    "    def __init__(self, numChannels, classes, ks):\n",
    "        # call the parent constructor\n",
    "        super(cnn, self).__init__()\n",
    "        # initialize first set of CONV => POOL => RELU layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=200,\n",
    "            kernel_size=ks, stride=2, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # initialize second set of CONV => POOL => RELU layers\n",
    "        self.dropout1 = nn.Dropout(0.6)\n",
    "        self.conv2 = nn.Conv2d(in_channels=200, out_channels=650,\n",
    "            kernel_size=ks, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "                               \n",
    "        # initialize final set of CONV => POOL => RELU layers\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.conv3 = nn.Conv2d(in_channels=650, out_channels=350,\n",
    "            kernel_size=ks, padding=1)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "                               \n",
    "        # initialize our softmax classifier\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(in_features=(3150 if ks[0]==3 else 1400), out_features=128)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "            \n",
    "        self.dropout4 = nn.Dropout(0.6)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=classes)\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => RELU => POOL layers\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x) \n",
    "        \n",
    "        # through final CONV => RELU => POOL layer\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        m = nn.Flatten()\n",
    "        x = m(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.logSoftmax(x)\n",
    "        # return the output predictions\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77a009b-3438-4742-ae23-2da2baf7f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy function\n",
    "def accuracy(pred, y):\n",
    "    _, predictions = pred.max(1)\n",
    "    correct = (predictions == y).sum()\n",
    "    samples = predictions.size(0)\n",
    "    return correct/samples\n",
    "\n",
    "# training function\n",
    "def train(model, optim, lossFn, loader_train, loader_val, epochs=50, device='cpu'):\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for e in (t := trange(epochs)):\n",
    "\n",
    "        # train\n",
    "        epochloss=0; epochacc=0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for _, (x,y) in enumerate(loader_train):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # perform a forward pass and calculate the training loss\n",
    "            # zero out the gradients\n",
    "            optim.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = lossFn(pred,y)\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            # find acc\n",
    "            acc = accuracy(pred, y)\n",
    "            # update weights\n",
    "            optim.step()\n",
    "            epochloss += loss.item()\n",
    "            epochacc += acc.item()\n",
    "        train_loss = epochloss/len(loader_train)\n",
    "        train_acc = epochacc/len(loader_train)\n",
    "\n",
    "        # evaluate\n",
    "        epochloss=0; epochacc=0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _, (x,y) in enumerate(loader_val):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                # make the predictions\n",
    "                pred = model(x)\n",
    "                loss = lossFn(pred,y)\n",
    "                # find acc\n",
    "                acc = accuracy(pred, y)\n",
    "                epochloss += loss.item()\n",
    "                epochacc += acc.item()\n",
    "        val_loss = epochloss/len(loader_val)\n",
    "        val_acc = epochacc/len(loader_val)\n",
    "\n",
    "        # add to lists\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        t.set_description('loss %.2f acc %.3f' % (val_loss, val_acc))\n",
    "    \n",
    "    model.to(torch.device('cpu'))\n",
    "    return train_losses, train_accs, val_losses, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f8908c-bfd1-41fc-8701-02bd8bd6ae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1\n",
      "The training set has  4646  samples\n",
      "Their index locations are:  [   0    1    2 ... 6190 6191 6194] \n",
      "\n",
      "The validation set has  1549  samples\n",
      "Their index locations are:  [   5    7    8 ... 6188 6192 6193] \n",
      "\n",
      "\n",
      "Fold  2\n",
      "The training set has  4646  samples\n",
      "Their index locations are:  [   0    3    4 ... 6192 6193 6194] \n",
      "\n",
      "The validation set has  1549  samples\n",
      "Their index locations are:  [   1    2    6 ... 6183 6186 6187] \n",
      "\n",
      "\n",
      "Fold  3\n",
      "The training set has  4646  samples\n",
      "Their index locations are:  [   1    2    4 ... 6192 6193 6194] \n",
      "\n",
      "The validation set has  1549  samples\n",
      "Their index locations are:  [   0    3   13 ... 6185 6190 6191] \n",
      "\n",
      "\n",
      "Fold  4\n",
      "The training set has  4647  samples\n",
      "Their index locations are:  [   0    1    2 ... 6191 6192 6193] \n",
      "\n",
      "The validation set has  1548  samples\n",
      "Their index locations are:  [   4    9   11 ... 6182 6189 6194] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4-Fold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "k = 4 # number of folds\n",
    "kf = KFold(n_splits=k,shuffle=True)\n",
    "# split training feature matrix into training and validation sets\n",
    "f=1\n",
    "for train_index, validation_index in kf.split(X_train):\n",
    "    print('Fold ',f)\n",
    "    print('The training set has ', train_index.shape[0],' samples')\n",
    "    print('Their index locations are: ', train_index, '\\n')\n",
    "    print('The validation set has ', validation_index.shape[0],' samples')\n",
    "    print('Their index locations are: ', validation_index,'\\n\\n')\n",
    "    f+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d1a5f7-0f63-4a3f-9ebb-a0f096294fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.13 acc 0.961: 100%|██████████| 100/100 [03:32<00:00,  2.13s/it]\n",
      "loss 0.18 acc 0.955: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.16 acc 0.958: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.18 acc 0.957: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "<ipython-input-7-2254af38d13d>:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  feats = np.array([lr,ks,bs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.20 acc 0.953: 100%|██████████| 100/100 [03:23<00:00,  2.03s/it]\n",
      "loss 0.21 acc 0.952: 100%|██████████| 100/100 [03:23<00:00,  2.03s/it]\n",
      "loss 0.18 acc 0.957: 100%|██████████| 100/100 [03:23<00:00,  2.03s/it]\n",
      "loss 0.17 acc 0.961: 100%|██████████| 100/100 [03:23<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.15 acc 0.964: 100%|██████████| 100/100 [03:19<00:00,  1.99s/it]\n",
      "loss 0.18 acc 0.963: 100%|██████████| 100/100 [03:19<00:00,  1.99s/it]\n",
      "loss 0.25 acc 0.948: 100%|██████████| 100/100 [03:19<00:00,  1.99s/it]\n",
      "loss 0.15 acc 0.962: 100%|██████████| 100/100 [03:19<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.22 acc 0.958: 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n",
      "loss 0.18 acc 0.962: 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n",
      "loss 0.19 acc 0.962: 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n",
      "loss 0.19 acc 0.960: 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.19 acc 0.953: 100%|██████████| 100/100 [05:54<00:00,  3.54s/it]\n",
      "loss 0.13 acc 0.969: 100%|██████████| 100/100 [05:54<00:00,  3.54s/it]\n",
      "loss 0.26 acc 0.938: 100%|██████████| 100/100 [05:54<00:00,  3.54s/it]\n",
      "loss 0.15 acc 0.955: 100%|██████████| 100/100 [05:54<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.15 acc 0.971: 100%|██████████| 100/100 [05:16<00:00,  3.16s/it]\n",
      "loss 0.16 acc 0.965: 100%|██████████| 100/100 [05:16<00:00,  3.16s/it]\n",
      "loss 0.23 acc 0.954: 100%|██████████| 100/100 [05:16<00:00,  3.16s/it]\n",
      "loss 0.16 acc 0.964: 100%|██████████| 100/100 [05:16<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.18 acc 0.961: 100%|██████████| 100/100 [03:51<00:00,  2.32s/it]\n",
      "loss 0.13 acc 0.966: 100%|██████████| 100/100 [03:51<00:00,  2.31s/it]\n",
      "loss 0.19 acc 0.968: 100%|██████████| 100/100 [03:51<00:00,  2.32s/it]\n",
      "loss 0.17 acc 0.969: 100%|██████████| 100/100 [03:51<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.24 acc 0.958: 100%|██████████| 100/100 [03:26<00:00,  2.07s/it]\n",
      "loss 0.23 acc 0.965: 100%|██████████| 100/100 [03:26<00:00,  2.07s/it]\n",
      "loss 0.25 acc 0.955: 100%|██████████| 100/100 [03:26<00:00,  2.07s/it]\n",
      "loss 0.14 acc 0.971: 100%|██████████| 100/100 [03:26<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.18 acc 0.933: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.15 acc 0.955: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.22 acc 0.943: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.15 acc 0.951: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.21 acc 0.949: 100%|██████████| 100/100 [03:23<00:00,  2.03s/it]\n",
      "loss 0.20 acc 0.953: 100%|██████████| 100/100 [03:23<00:00,  2.03s/it]\n",
      "loss 0.23 acc 0.963: 100%|██████████| 100/100 [03:23<00:00,  2.03s/it]\n",
      "loss 0.25 acc 0.958: 100%|██████████| 100/100 [03:23<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.19 acc 0.957: 100%|██████████| 100/100 [03:19<00:00,  1.99s/it]\n",
      "loss 0.21 acc 0.963: 100%|██████████| 100/100 [03:19<00:00,  1.99s/it]\n",
      "loss 0.20 acc 0.959: 100%|██████████| 100/100 [03:19<00:00,  1.99s/it]\n",
      "loss 0.18 acc 0.960: 100%|██████████| 100/100 [03:19<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.15 acc 0.964: 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n",
      "loss 0.26 acc 0.952: 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n",
      "loss 0.18 acc 0.960: 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n",
      "loss 0.29 acc 0.954: 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.14 acc 0.962: 100%|██████████| 100/100 [05:54<00:00,  3.54s/it]\n",
      "loss 0.10 acc 0.974: 100%|██████████| 100/100 [05:53<00:00,  3.53s/it]\n",
      "loss 0.25 acc 0.950: 100%|██████████| 100/100 [05:53<00:00,  3.54s/it]\n",
      "loss 0.13 acc 0.968: 100%|██████████| 100/100 [05:53<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.20 acc 0.962: 100%|██████████| 100/100 [05:15<00:00,  3.16s/it]\n",
      "loss 0.20 acc 0.958: 100%|██████████| 100/100 [05:15<00:00,  3.15s/it]\n",
      "loss 0.19 acc 0.963: 100%|██████████| 100/100 [05:15<00:00,  3.16s/it]\n",
      "loss 0.17 acc 0.956: 100%|██████████| 100/100 [05:15<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.23 acc 0.958: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n",
      "loss 0.14 acc 0.967: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n",
      "loss 0.22 acc 0.960: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n",
      "loss 0.23 acc 0.963: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.15 acc 0.959: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n",
      "loss 0.17 acc 0.974: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n",
      "loss 0.29 acc 0.966: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n",
      "loss 0.20 acc 0.965: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.15 acc 0.952: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]\n",
      "loss 0.14 acc 0.964: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]\n",
      "loss 0.23 acc 0.956: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]\n",
      "loss 0.31 acc 0.939: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.21 acc 0.957: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.19 acc 0.950: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.18 acc 0.963: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.18 acc 0.965: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.15 acc 0.956: 100%|██████████| 100/100 [03:18<00:00,  1.99s/it]\n",
      "loss 0.19 acc 0.954: 100%|██████████| 100/100 [03:18<00:00,  1.99s/it]\n",
      "loss 0.17 acc 0.961: 100%|██████████| 100/100 [03:18<00:00,  1.99s/it]\n",
      "loss 0.22 acc 0.953: 100%|██████████| 100/100 [03:18<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.16 acc 0.969: 100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n",
      "loss 0.25 acc 0.951: 100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n",
      "loss 0.16 acc 0.965: 100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n",
      "loss 0.24 acc 0.957: 100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.12 acc 0.971: 100%|██████████| 100/100 [05:53<00:00,  3.54s/it]\n",
      "loss 0.17 acc 0.945: 100%|██████████| 100/100 [05:53<00:00,  3.54s/it]\n",
      "loss 0.15 acc 0.965: 100%|██████████| 100/100 [05:53<00:00,  3.53s/it]\n",
      "loss 0.13 acc 0.961: 100%|██████████| 100/100 [05:53<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.14 acc 0.959: 100%|██████████| 100/100 [05:15<00:00,  3.15s/it]\n",
      "loss 0.14 acc 0.954: 100%|██████████| 100/100 [05:15<00:00,  3.15s/it]\n",
      "loss 0.19 acc 0.947: 100%|██████████| 100/100 [05:15<00:00,  3.15s/it]\n",
      "loss 0.15 acc 0.958: 100%|██████████| 100/100 [05:15<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.25 acc 0.959: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n",
      "loss 0.12 acc 0.967: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n",
      "loss 0.17 acc 0.965: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n",
      "loss 0.16 acc 0.963: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.16 acc 0.962: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n",
      "loss 0.30 acc 0.969: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n",
      "loss 0.24 acc 0.951: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n",
      "loss 0.17 acc 0.964: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.16 acc 0.957: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]\n",
      "loss 0.18 acc 0.958: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]\n",
      "loss 0.10 acc 0.970: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]\n",
      "loss 0.17 acc 0.941: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.18 acc 0.962: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.18 acc 0.946: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.24 acc 0.961: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "loss 0.15 acc 0.955: 100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.21 acc 0.956: 100%|██████████| 100/100 [03:18<00:00,  1.99s/it]\n",
      "loss 0.23 acc 0.943: 100%|██████████| 100/100 [03:18<00:00,  1.99s/it]\n",
      "loss 0.13 acc 0.968: 100%|██████████| 100/100 [03:18<00:00,  1.99s/it]\n",
      "loss 0.14 acc 0.966: 100%|██████████| 100/100 [03:18<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.26 acc 0.946: 100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n",
      "loss 0.19 acc 0.948: 100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n",
      "loss 0.33 acc 0.946: 100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n",
      "loss 0.18 acc 0.957: 100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.16 acc 0.972: 100%|██████████| 100/100 [05:53<00:00,  3.53s/it]\n",
      "loss 0.14 acc 0.961: 100%|██████████| 100/100 [05:53<00:00,  3.53s/it]\n",
      "loss 0.22 acc 0.944: 100%|██████████| 100/100 [05:53<00:00,  3.53s/it]\n",
      "loss 0.17 acc 0.960: 100%|██████████| 100/100 [05:53<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.16 acc 0.953: 100%|██████████| 100/100 [05:15<00:00,  3.15s/it]\n",
      "loss 0.14 acc 0.963: 100%|██████████| 100/100 [05:15<00:00,  3.15s/it]\n",
      "loss 0.32 acc 0.947: 100%|██████████| 100/100 [05:15<00:00,  3.15s/it]\n",
      "loss 0.16 acc 0.955: 100%|██████████| 100/100 [05:15<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.14 acc 0.966: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n",
      "loss 0.18 acc 0.952: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n",
      "loss 0.17 acc 0.957: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n",
      "loss 0.30 acc 0.957: 100%|██████████| 100/100 [03:50<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32/32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.28 acc 0.961: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n",
      "loss 0.19 acc 0.954: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n",
      "loss 0.39 acc 0.955: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n",
      "loss 0.16 acc 0.967: 100%|██████████| 100/100 [03:26<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# define values to loop over\n",
    "learning_rates = [0.0004, 0.0006, 0.0008, 0.001]\n",
    "kernel_sizes = [(3,3),(5,5)]\n",
    "batch_sizes = [256, 128, 64, 32]\n",
    "\n",
    "# setup device to be cuda if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# choosing values with highest\n",
    "topmax_avg_acc = np.zeros(32)\n",
    "max_avg_stddevs = np.zeros(32)\n",
    "best = np.zeros((32,3))\n",
    "\n",
    "count = 0\n",
    "for lr in learning_rates:\n",
    "    for ks in kernel_sizes:\n",
    "        for bs in batch_sizes:\n",
    "    \n",
    "            sub_accs = []\n",
    "            count +=1\n",
    "            print(f'Iteration {count}/{len(learning_rates)*len(kernel_sizes)*len(batch_sizes)}:')\n",
    "            for train_index,validation_index in kf.split(X_train):\n",
    "\n",
    "                # select training set using the indices found from kf.split\n",
    "                X_train2, X_val = X_train[train_index], X_train[validation_index]\n",
    "                # select validation set using the indices found from kf.split\n",
    "                t_train2, t_val = t_train[train_index], t_train[validation_index]\n",
    "\n",
    "                X_train2 = torch.tensor(X_train2, dtype=torch.float32).cuda()\n",
    "                X_val = torch.tensor(X_val, dtype=torch.float32).cuda()\n",
    "                t_train2 = torch.tensor(t_train2, dtype=torch.long).cuda()\n",
    "                t_val = torch.tensor(t_val, dtype=torch.long).cuda()\n",
    "\n",
    "                # set data loaders for train and val sets\n",
    "                train_data = TensorDataset(X_train2, t_train2)\n",
    "                val_data = TensorDataset(X_val, t_val)\n",
    "                X_train2 = []; t_train2 = []\n",
    "                loader_train = DataLoader(train_data, shuffle=True, batch_size=bs)\n",
    "                loader_val = DataLoader(val_data, shuffle=True, batch_size=bs)\n",
    "                train_data = []; val_data = [] # free space\n",
    "\n",
    "                # initialize the model\n",
    "                model = cnn(3,10,ks)\n",
    "                # initialize our optimizer and loss function\n",
    "                lossFn = nn.CrossEntropyLoss()\n",
    "                optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                # Train model\n",
    "                _, _, _, val_accs = train(model,optim,lossFn,loader_train,loader_val,EPOCHS,device)\n",
    "                # Performance Measure\n",
    "                sub_accs.append(val_accs[len(val_accs)-1])\n",
    "\n",
    "            avg_acc_val = np.sum(np.array(sub_accs,dtype=np.float32))/k\n",
    "            val_acc_stddev = np.std(np.array(sub_accs, dtype=np.float32))\n",
    "            feats = np.array([lr,ks,bs])\n",
    "            #print(f'sub_accs:\\n {sub_accs}\\n\\navg_acc_val:\\n {avg_acc_val}\\n\\nfeats:\\n {feats}')\n",
    "            #print('average accuracy: ', avg_acc_val,'\\n')\n",
    "            #accs += [avg_acc_val]\n",
    "            \n",
    "            for m in range(len(topmax_avg_acc)):\n",
    "                if avg_acc_val >= topmax_avg_acc[m]:\n",
    "                    if m==31:\n",
    "                        best = np.vstack([best, feats])[1:]\n",
    "                        topmax_avg_acc = np.append(topmax_avg_acc,avg_acc_val)[1:]\n",
    "                        max_avg_stddevs = np.append(max_avg_stddevs,val_acc_stddev)[1:]\n",
    "\n",
    "                    else:\n",
    "                        continue\n",
    "                elif avg_acc_val < topmax_avg_acc[m]: \n",
    "                    if m == 0:\n",
    "                        break\n",
    "                    if m != 0:\n",
    "                        topmax_avg_acc = np.append(np.append(topmax_avg_acc[:m],avg_acc_val),topmax_avg_acc[m:])[1:]\n",
    "                        max_avg_stddevs = np.append(np.append(max_avg_stddevs[:m],val_acc_stddev),max_avg_stddevs[m:])[1:]\n",
    "                        best = np.vstack([np.vstack([best[:m],feats]),best[m:]])[1:]\n",
    "                        break\n",
    "            #print(f'\\ntopmax_avg_acc:\\n {topmax_avg_acc}\\n\\nbest:\\n {best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5860e5a1-7658-4d25-ba26-217aa11e5914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy and feature rankings:\n",
      "acc, std_dev, lr, ks, bs\n",
      "\n",
      "[[0.9454376697540283 0.008405357599258423 0.0006 (3, 3) 256]\n",
      " [0.9494824409484863 0.004553064703941345 0.001 (3, 3) 32]\n",
      " [0.9526563882827759 0.009018884971737862 0.0008 (3, 3) 256]\n",
      " [0.9535328149795532 0.010953939519822598 0.0004 (5, 5) 256]\n",
      " [0.954272985458374 0.004525352735072374 0.0008 (5, 5) 128]\n",
      " [0.9543731808662415 0.005717592779546976 0.001 (5, 5) 128]\n",
      " [0.9556213021278381 0.00561443530023098 0.0006 (3, 3) 128]\n",
      " [0.9557253122329712 0.003472384996712208 0.0004 (3, 3) 128]\n",
      " [0.9557651877403259 0.0029758550226688385 0.0008 (3, 3) 64]\n",
      " [0.9560989737510681 0.006363527849316597 0.001 (3, 3) 128]\n",
      " [0.9565662145614624 0.010187032632529736 0.001 (3, 3) 256]\n",
      " [0.9575688242912292 0.004712746012955904 0.0006 (3, 3) 32]\n",
      " [0.9578682780265808 0.0024163571652024984 0.0004 (3, 3) 256]\n",
      " [0.9579687714576721 0.005198695696890354 0.001 (5, 5) 64]\n",
      " [0.9584975838661194 0.009879971854388714 0.001 (3, 3) 64]\n",
      " [0.9587301015853882 0.006080074701458216 0.0008 (3, 3) 128]\n",
      " [0.9591907262802124 0.006357782520353794 0.0004 (3, 3) 64]\n",
      " [0.9592695236206055 0.005045643541961908 0.001 (5, 5) 32]\n",
      " [0.9594887495040894 0.010053410194814205 0.001 (5, 5) 256]\n",
      " [0.9596816897392273 0.003010103479027748 0.0006 (5, 5) 128]\n",
      " [0.9596995115280151 0.001984112896025181 0.0006 (3, 3) 64]\n",
      " [0.9602383971214294 0.006912877783179283 0.0008 (3, 3) 32]\n",
      " [0.9603856205940247 0.0014588094782084227 0.0004 (3, 3) 32]\n",
      " [0.9606047868728638 0.009868841618299484 0.0008 (5, 5) 256]\n",
      " [0.9614158868789673 0.006696430034935474 0.0008 (5, 5) 32]\n",
      " [0.9620535373687744 0.006191481836140156 0.0004 (5, 5) 32]\n",
      " [0.9620552659034729 0.003303761361166835 0.0006 (5, 5) 64]\n",
      " [0.9633520841598511 0.008778642863035202 0.0006 (5, 5) 256]\n",
      " [0.9633875489234924 0.006011290475726128 0.0004 (5, 5) 128]\n",
      " [0.9636057615280151 0.002754902234300971 0.0008 (5, 5) 64]\n",
      " [0.9658923149108887 0.005440402310341597 0.0006 (5, 5) 32]\n",
      " [0.9662621021270752 0.003093661041930318 0.0004 (5, 5) 64]]\n"
     ]
    }
   ],
   "source": [
    "# print cv results\n",
    "print(f'accuracy and feature rankings:\\nacc, std_dev, lr, ks, bs\\n\\n{np.hstack([topmax_avg_acc[:,np.newaxis],max_avg_stddevs[:,np.newaxis],best])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eafaa1-a33c-4988-8db0-44e9801ef3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC-PyTorch-1.9",
   "language": "python",
   "name": "ngc-pytorch-1.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
